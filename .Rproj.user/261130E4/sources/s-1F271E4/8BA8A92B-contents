# Disable GPU                                                                   
# Sys.setenv("CUDA_VISIBLE_DEVICES" = -1)                                     
# Sys.setenv("TF_DETERMINISTIC_OPS" = 1)
# keras::install_keras(tensorflow = "gpu")
# tfdeterminism
# reticulate::py_install("tensorflow-determinism",pip =TRUE)

library(reticulate)
repl_python()
from tfdeterminism import patch
patch()
exit



library(keras)
seed = 42



# keras::install_keras(tensorflow = "2.1.0-gpu")
# tensorflow::install_tensorflow(version = '2.1.0-gpu')

# reticulate::py_config()

# use_session_with_seed(seed)
pacman::p_load(GGally, freeknotsplines, lattice, plyr, locpol, splines, ranger, spm, plotly, pdist,pheatmap, DiscriMiner, pROC, caret, wesanderson)
remove_outlier = function(v){
  out = boxplot.stats(v)$out
  return(list(value = v[!v%in%out],index = which(v%in%out)))
}
RSD = function(data){
  return(apply(data,1,function(x){
    x = remove_outlier(x)[[1]]
    return(sd(x,na.rm=T)/mean(x,na.rm=T))
  }))
}
robust_sd = function(x){
  x = remove_outlier(x)[[1]]
  sd(x,na.rm = TRUE)
}
normalize_batch_effect = function(d, batch){ # d=e
  
  return(t(apply(t(d),2,function(x){
    x/(rep(by(x,batch,mean), table(batch))/mean(x))
  })))
  
}


# scatter plot
scatter_plot = F
source("https://raw.githubusercontent.com/slfan2013/rcodes/master/read_data.R")

# data_file = "SHS4e_GCBinBase_nosum_Zhang_smaller batch&istd_prederrf_20200320.xlsx"
# data_file = "P20 positive mode.xlsx"
# data_file = "P20 negative mode.xlsx"
# data_file = "datasets//ADNI GC.xlsx"
# data_file = "datasets//gc_xinchen.csv"
# data_file = "519117_Zhu_CSH_Submit_reSERRF.xlsx"
# data_file = "519117_Zhu_negCSH_ToBeSERRFed.csv"
# data_file = "519117_Zhu_posCSH_toBeSERRF_2batches.csv"
# data_file = "530103_Zhu_negCSH_raw_ToBeNormalized.csv"
# data_file = "530103_Zhu_posCSH_processed_ToBeNormalized.csv"
# data_file = "530103_Zhu_negCSH_processed_ToBeNormalized.csv"
# data_file = "96wellplatevalidation_mammalplasma_GC.xlsx"
# data_file = "530103_Zhu_negCSH_withFA_ToBeNormalized.csv"
data_file = "519944_Zhu_posHILIC_toBeSERRF.csv"
# data_file = "530103_Zhu_negCSH_uncombinedadducts_withFA_ToBeNormalized.csv"
# data_file = "SubmitforNormalization_GC_06082020.xlsx"



if(grepl("\\\\",data_file)){
  comp = strsplit(data_file,"\\\\")[[1]]
}else{
  comp = strsplit(data_file,"/")[[1]]
}

filename = gsub("\\.csv|\\.xlsx","",comp[length(comp)])
# root = paste0(paste0(comp[-length(comp)],collapse = "\\"),"\\")
root = ""
dir = paste0(root,filename," - SERDA result")
dir.create(dir)



data = read_data(data_file)

f = data$f
p = data$p
e = data$e_matrix

e_na = e[,is.na(p$sampleType)]
p_na = p[is.na(p$sampleType),]

e = e[,!is.na(p$sampleType)]
p = p[!is.na(p$sampleType),]

original_time = as.numeric(p$time)

# p$time = c(1:length(p$time))[rank(original_time)]!!!
# e = e[,order(p$time)]
# p = p[order(p$time),]

# real_validate = p$sampleType == 'validate1' 
# p$sampleType[p$sampleType == 'validate1'] = "sample"


# faking the biological_group.
# p$biological_group[sample_index] = sample(p$biological_group[sample_index])
# for(i in 1:nrow(e)){
#   
#   if(runif(1)<0.3){
#     if(runif(1)<0.5){
#       e[i,p$biological_group %in% "M"] = e[i,p$biological_group %in% "M"] + rnorm(length(e[i,p$biological_group %in% "M"]), mean = mean(e[i,p$biological_group %in% "M"], na.rm = TRUE)*.2, sd = sd(e[i,p$biological_group %in% "M"], na.rm = TRUE)*0.1)
#     }else{
#       e[i,p$biological_group %in% "M"] = e[i,p$biological_group %in% "M"] - rnorm(length(e[i,p$biological_group %in% "M"]), mean = mean(e[i,p$biological_group %in% "M"], na.rm = TRUE)*.2, sd = sd(e[i,p$biological_group %in% "M"], na.rm = TRUE)*0.1)
#     }
#   }
#   
# 
#   
# }






# pca = prcomp(t(e), scale. = TRUE)
# # Ellipse by groups
# pca_data = data.table(x = pca$x[,1], y = pca$x[,2], group = p$biological_group, sampleType = p$sampleType)
# ggplot(pca_data, aes(x, y, color = group))+  geom_point()+ stat_ellipse()





# e = e[,p$batch == "A"]
# p = p[p$batch=="A",]
# for(i in 1:nrow(e)){
#   e[i,e[i,]<1] = 1
# }

transform = function(e, forward = TRUE, y0 = NULL, lambda = NULL, regular_log = FALSE){
  
  
  if(is.null(y0)){y0=0}
  
  if(class(e) == 'numeric'){
    
    if(forward){
      
      
    }else{
      if(is.null(lambda)){
        stop("You forgot to input lambda.")
      }
      e_after = 0.5 * (2*y0 - lambda * exp(-e) + exp(e))
      
    }
    
  }else{
    
    if(forward){
      if(regular_log){
        e_after = log(e)
      }else{
        e_after = e
        lambda = rowMeans(e)^2
        for(i in 1:nrow(e)){
          e_after[i,] = log(e[i,] - y0 + sqrt((e[i,]-y0)^2 + lambda[i]))
        }
      }
    }else{
      if(regular_log){
        e_after = exp(e)
      }else{
        if(is.null(lambda)){
          stop("You forgot to input lambda.")
        }
        e_after = e
        for(i in 1:nrow(e)){
          e_after[i,] = 0.5 * (2*y0 - lambda[i] * exp(-e[i,]) + exp(e[i,]))
        }
      }
    }
    
  }
  
  
  return(list(e_after, lambda = lambda))
}

e = transform(e)
lambda = e$lambda
e = e[[1]]





qc_index = which(p$sampleType == 'qc')
# How number of QCs influence the performance of normalization.
# select_index = (qc_index[round(length(qc_index)/3)]+1):qc_index[round(length(qc_index)/3*2)]
# e = e[,select_index]
# p = p[select_index,]
# qc_index = which(p$sampleType == 'qc')

with_validates = any(grepl("validate", p$sampleType))
if(with_validates){
  validates = unique(p$sampleType[grepl("validate", p$sampleType)])
  validates_indexes = list()
  for(i in 1:length(validates)){
    validates_indexes[[i]] = which(p$sampleType %in% validates[i])
  }
  names(validates_indexes) = validates
}else{
  validates = NULL
}


# Even without validate, having this for cross-validation for selecting best hyperparameters.
p_biological_group = c()
# Even without validate, having this for cross-validation for selecting best hyperparameters.
auc_biological_group = c()

sample_index = which(p$sampleType == 'sample')
batch_index = p$batch


with_biological_group = "biological_group" %in% colnames(p)



scale_data = function(d){# d = e. 
  sds = apply(d, 1, sd)
  
  # !!! how to deal with zero sds.
  sds[sds == 0] = 1
  
  
  means = apply(d, 1, mean)
  data_scale = d
  for(i in 1:nrow(d)){
    data_scale[i,] = (d[i,] - means[i])/sds[i]
  }
  return(list(
    data_scale = data_scale,
    means = means,
    sds = sds
  ))
}

# How distance of QCs influence the performance of normalization.
# qc_index = qc_index[(1:length(qc_index))%%5==0]


e_qc = e[,qc_index]
p_qc = p[qc_index,]
if(with_validates){
  e_validates = list()
  for(i in 1:length(validates)){
    e_validates[[i]] = e[,which(p$sampleType %in% validates[i])]
  }
}

# Even without validate, having this for cross-validation for selecting best hyperparameters.
validates_RSDs = list()
if(with_validates){
  e_validates_scale = list()
  for(i in 1:length(validates)){
    
    e_validates_scale[[i]] = scale_data(e_validates[[i]])
    
  }
  validates_RSDs[[i]] = 1
}




x_sample_scale = scale_data(e[,sample_index])
x_sample_scale_d = t(x_sample_scale$data_scale)
x_sample_scale_sd = x_sample_scale$sds
x_sample_scale_mean = x_sample_scale$means


x_qc_scale = scale_data(e[,qc_index])
x_qc_scale_d = t(x_qc_scale$data_scale)
x_qc_scale_sd = x_qc_scale$sds
x_qc_scale_mean = x_qc_scale$means



# calculation time
calculation_time = c()


# x_train = t(e_qc)
# # 8/2 split
# set.seed(42)
# s_index = sample(1:nrow(x_train), size = nrow(x_train) * 0.8)
# t_index = (1:nrow(x_train))[!(1:nrow(x_train)) %in% s_index]
# 
# s_index = sort(s_index)
# t_index = sort(t_index)
# 
# x_test = x_train[t_index,]
# x_train = x_train[s_index,]


# pca = prcomp(x_train)

patience = 50; layer_units = c(1400); batch_size = 128; epochs = 2000; verbose = 0;activations = c("elu"); drop_out_rate = 0.05; optimizer = "adam";s_index = NULL; t_index = NULL
# ae
ae_model = function(x_train_input, x_test_input,x_train_output, x_test_output, patience = 50, layer_units = c(1400), batch_size = 128, epochs = 2000, verbose = 0,activations = c("elu"), drop_out_rate = 0.05, optimizer = "adam",s_index = NULL, t_index = NULL
                    # , customize_performance_evaluation_set
){
  # seed = 42
  # reticulate::py_config()
  # reticulate::py_set_seed(seed)
  # set.seed(seed)
  
  
  # middles = list()
  # tensorflow::tf$random$set_seed(42)
  # middles[[length(layer_units)+1]] = keras_model_sequential() # here is the final layer.
  # middles[[length(layer_units)+1]] %>% layer_dense(input_shape = layer_units[length(layer_units)], units = nrow(f),use_bias = TRUE)
  # 
  # for(layer_index in length(layer_units):1){
  #   tensorflow::tf$random$set_seed(42)
  #   middles[[layer_index]] = keras_model_sequential(name = paste0('middle',layer_index))
  #   if(layer_index==1){
  #     middles[[layer_index]] %>% layer_dense(input_shape = nrow(f), units = layer_units[layer_index], activation = activations[layer_index],use_bias = TRUE) # this is the first mid-layer
  #   }else{
  #     middles[[layer_index]] %>% layer_dense(input_shape = layer_units[layer_index-1], units = layer_units[layer_index], activation = activations[layer_index],use_bias = TRUE) # this is other mid-layer
  #   }
  # }
  # model = middles[[1]]%>%layer_dropout(drop_out_rate)
  # for(i in 2:length(middles)){
  #   model_temp = middles[[i]]
  #   model %>% model_temp
  # }
  
  early_stopping <- callback_early_stopping(patience = patience)
  tensorflow::tf$random$set_seed(42)
  model <- keras_model_sequential() 
  # model <- keras_model_sequential() 
  model %>% 
    layer_dense(units = layer_units, activation =  activations, input_shape = nrow(f)) %>% 
    layer_dropout(rate = drop_out_rate) %>% 
    layer_dense(units = nrow(f)
                # , activation = 'softmax'
    )
  
  model %>% compile(
    loss = "mean_absolute_error",#1997171
    # loss = "mean_squared_error",#0.2111578
    # loss = "mean_absolute_percentage_error",#0.3923684
    # loss = "mean_squared_logarithmic_error",#0.2652711
    # loss = "squared_hinge",#1.961605
    # loss = "hinge",#3.870686
    # loss = "logcosh",#0.2002801
    # loss = "huber_loss",#0.2005046
    # loss = "categorical_crossentropy",#2.752495
    # loss = "sparse_categorical_crossentropy",#error
    # loss = "binary_crossentropy",#1.201967
    # loss = "kullback_leibler_divergence",#2.403364
    # loss = "poisson",#error
    # loss = "cosine_proximity",#error
    optimizer = optimizer
  )
  model %>% save_model_hdf5("model.h5")
  
  # Winsorizing
  # for(i in 1:ncol(x_train)){
  #   
  #   quan = quantile(x_train[,i], probs = c(0.01,0.99))
  #   
  # }
  
  
  model %>% fit(
    x = x_train_input,
    y = x_train_output,
    epochs = epochs,
    verbose = 2,
    batch_size = batch_size,
    validation_data = list(x_test_input, x_test_output), 
    callbacks = list(early_stopping)
  )
  
  epoch_n = length(model$history$epoch)#-patience
  # epoch_n  
  
  
  
  x_all_input = rbind(x_train_input,x_test_input)
  x_all_output = rbind(x_train_output,x_test_output)
  
  final_model <- load_model_hdf5("model.h5")
  final_model %>% fit(
    x = x_all_input,
    y = x_all_output,
    epochs = epoch_n,
    verbose = 2,
    batch_size = batch_size
  ) 
  
  
  
  
  
  
  if(is.null(s_index)){
    return(list(
      final_model = final_model,
      model = model
    ))
  }else{
    x_all = rbind(x_train_output,x_test_output)
    pred_train = rbind(x_train_output,x_test_output)
    
    model2 <- load_model_hdf5("model.h5")
    model2 %>% fit(
      x = pred_train,
      y = pred_train,
      epochs = epoch_n,
      verbose = verbose,
      batch_size = batch_size
    ) 
    
    
    pred = predict(model2,  x_all)
    
    x_all[s_index,] = pred[1:nrow(x_train_output),]
    x_all[t_index,] = pred[(nrow(x_train_output)+1):nrow(pred),]
    
    
    
    return(list(
      final_model = final_model,
      pred_train = x_all,
      model = model
    ))
  }
  

}


layer_units_options = expand.grid(x = seq(400, min(nrow(e)*2,1600), by = 200), activation_function = c("elu"), drop_rates = c(3:6)/100, patience = c(25), many_training = c(TRUE,FALSE),many_training_n = 500, noise_weight = 1, use_qc_sd_when_final_correct = c(T,F))
# layer_units_options = layer_units_options[1,]

l=1





# 5/11/2020 Between Batch effect is observed with Biorec and Sample, try batch-wise normalization. Batch effect on QC left samples are not observed.
# So, two options: 1; for each batch perform the SERDA, and then remove batch effect. 2; perform SERDA on all, then remove batch effect.
# Second is used.

# 5/15/2020 Check how to make batch_size optimal with calculation time.

# 5/16/2020 How to deal with outliers. # Maybe after checking after normalization, which samples were too far away but before normalization it was not too far away!!!
# 5/10/2020 Add normal noise and see the performance. See test.R for more information about noise.

# 5/24/2020 Try different way of making qc cvRSD lower. Maybe calculate RSD for each fold and then average?

# 6/12/2020; justfitying why normalize validate separately. The machine introduced different amoung of systematic error to the biorec. Biorec are exactly same sample, thus the distribution is different from sample, Thus how much variance add to the QC to meet the distribution of the target depends on the target. No matter what target we use, we always use QC to train the model and then applied to the target.


# 6/12/2020; can we use division instead of minus when applying normalization? For example, compound # 1570 in 519117 Zhu Pos mode.



warning_index = list()


QC_cv_RSDs = list()


for(l in 1:nrow(layer_units_options)){
  start = Sys.time()
  warning_index[[l]] = c("")
  x_left_normalize = x_left_predict = x_qc_scale_d
  sample_index_temp = 1:nrow(x_left_normalize)
  x_lefts = split(sample_index_temp, sample_index_temp%%3)
  
  
  rsds = c()
  for(i in 1:length(x_lefts)){
    print(i)
    x_train = t(e_qc[,-x_lefts[[i]]])
    x_left = t(e_qc[,x_lefts[[i]]])
    # add noise to input data according to train and target.
    if(layer_units_options$many_training[l]){
      index = (1:layer_units_options$many_training_n[l])%%nrow(x_train)
      index[index==0] = nrow(x_train)
      x_train_output = x_train_input = x_train[index,] #sample(size = many_training_n, 1:nrow(t(e_qc)), replace = TRUE)
    }else{
      x_train_output = x_train
      x_train_input = x_train
    }
    
    x_train_var = apply(x_train_input,2,robust_sd)^2
    target_var = apply(x_left,2,robust_sd)^2
    for(j in 1:ncol(x_train_input)){
      if(target_var[j] > x_train_var[j]){
        set.seed(j)
        x_train_input[,j] = x_train_input[,j]+rnorm(length(x_train_input[,j]), sd = sqrt(target_var[j] - x_train_var[j])*layer_units_options$noise_weight[l])
      }
    }
    
    
    
    x_train_input = t(scale_data(t(x_train_input))[[1]])
    x_train_output = t(scale_data(t(x_train_output))[[1]])
    
    
    # 8/2 split
    set.seed(42)
    s_index = sample(1:nrow(x_train_input), size = nrow(x_train_input) * 0.8)
    t_index = (1:nrow(x_train_input))[!(1:nrow(x_train_input)) %in% s_index]
    x_test_input = x_train_input[t_index,]
    x_train_input = x_train_input[s_index,]
    x_test_output = x_train_output[t_index,]
    x_train_output = x_train_output[s_index,]
    
    
    # layer_units = as.numeric(layer_units_options$x[l]); verbose = 0; patience = layer_units_options$patience[l]; activations = as.character(layer_units_options$activation_function[l]); drop_out_rate = layer_units_options$drop_rates[l]; optimizer = "adam"
    
    
    ae = ae_model(x_train_input, x_test_input,x_train_output, x_test_output, layer_units = as.numeric(layer_units_options$x[l]), verbose = 0, patience = layer_units_options$patience[l], activations = as.character(layer_units_options$activation_function[l]), drop_out_rate = layer_units_options$drop_rates[l], optimizer = "adam")
    
    
    
    final_model = ae$final_model
    
    
    x_left_scale = t(scale_data(t(x_left))[[1]])
    
    # x_left_predict[x_lefts[[i]],] = predict(final_model, x_left_scale)
    
    
    
    x_left_predict = predict(final_model, x_left_scale)
    
    
    # normalize;
    x_left_normalize = x_left_scale
    x_left_predict_o = x_left_predict
    for(j in 1:ncol(x_left_normalize)){
      x_left_normalize[,j] = x_left_scale[,j] - (x_left_predict[,j] - mean(x_left_predict[,j]))
      x_left_normalize[,j] = x_left_normalize[,j] * x_qc_scale_sd[j] + x_qc_scale_mean[j]
    }
    # normalize batch effect
    x_left_normalize = t(normalize_batch_effect(t(x_left_normalize),batch = p_qc$batch[x_lefts[[i]]]))
    
    # EXP
    # x_left_normalize_exp_current = exp(x_left_normalize)
    x_left_normalize_exp_current = t(transform(t(x_left_normalize), forward = FALSE, lambda = lambda)[[1]])
    
    
    # raw_means = apply(exp(e_qc),1, mean)
    raw_means = apply(transform(e_qc, forward = FALSE, lambda = lambda)[[1]],1, mean)
    norm_means = apply(x_left_normalize_exp_current,2, mean)
    
    # put normalized dataset to the original scale.
    tran_temp = transform(e_qc, forward = FALSE, lambda = lambda)[[1]][,x_lefts[[i]]]
    for(k in 1:ncol(x_left_normalize_exp_current)){
      mean_adj = x_left_normalize_exp_current[,k] - (norm_means[k] - raw_means[k])
      if(any(mean_adj<0) | is.na(any(mean_adj<0))){
        # cat(k,": ",sum(mean_adj<0),"\n")
        if(sum(mean_adj<0)>length(mean_adj)/2 | is.na(sum(mean_adj<0)>length(mean_adj)/2)){
          warning_index[[l]] = c(warning_index[[l]], paste0("qc",k))
          # mean_adj = exp(e_qc)[k,]
          mean_adj = tran_temp[k,]
        }else{
          mean_adj[mean_adj<0] = rnorm(sum(mean_adj<0), mean = min(mean_adj[mean_adj>0], na.rm = TRUE)/2, sd = min(mean_adj[mean_adj>0], na.rm = TRUE)/20)
        }  
      }
      x_left_normalize_exp_current[,k] = mean_adj
    }
    rsds[i] = median(RSD(t(x_left_normalize_exp_current)))
  }
  
  
  
  
  # # normalize;
  # x_left_predict_o = x_left_predict
  # for(i in 1:ncol(x_left_normalize)){
  #   x_left_normalize[,i] = x_qc_scale_d[,i] - (x_left_predict[,i] - mean(x_left_predict[,i]))
  #   x_left_normalize[,i] = x_left_normalize[,i] * x_qc_scale_sd[i] + x_qc_scale_mean[i]
  # }
  # # normalize batch effect
  # x_left_normalize = t(normalize_batch_effect(t(x_left_normalize),batch = p_qc$batch))
  # 
  # # EXP
  # # x_left_normalize_exp = exp(x_left_normalize)
  # x_left_normalize_exp = t(transform(t(x_left_normalize), forward = FALSE, lambda = lambda)[[1]])
  # 
  # 
  # # raw_means = apply(exp(e_qc),1, mean)
  # raw_means = apply(transform(e_qc, forward = FALSE, lambda = lambda)[[1]],1, mean)
  # norm_means = apply(x_left_normalize_exp,2, mean)
  # 
  # 
  # # put normalized dataset to the original scale.
  # tran_temp = transform(e_qc, forward = FALSE, lambda = lambda)[[1]]
  # for(k in 1:ncol(x_left_normalize_exp)){
  #   mean_adj = x_left_normalize_exp[,k] - (norm_means[k] - raw_means[k])
  #   if(any(mean_adj<0) | is.na(any(mean_adj<0))){
  #     
  #     # cat(k,": ",sum(mean_adj<0),"\n")
  #     
  #     if(sum(mean_adj<0)>length(mean_adj)/2 | is.na(sum(mean_adj<0)>length(mean_adj)/2)){
  #       warning_index[[l]] = c(warning_index[[l]], paste0("qc",k))
  #       # mean_adj = exp(e_qc)[k,]
  #       mean_adj = tran_temp[k,]
  #     }else{
  #       mean_adj[mean_adj<0] = rnorm(sum(mean_adj<0), mean = min(mean_adj[mean_adj>0], na.rm = TRUE)/2, sd = min(mean_adj[mean_adj>0], na.rm = TRUE)/20)
  #     }  
  #   }
  #   x_left_normalize_exp[,k] = mean_adj
  # }
  
  # x_lefts
  # qcs = c()
  # for(j in 1:length(x_lefts)){
  #   qcs[j] = median(RSD(t(x_left_normalize_exp[x_lefts[[j]],])))
  # }
  # 
  # x_left_normalize_exp 
  # 
  # QC_cv_RSDs[[l]] = median(RSD(t(x_left_normalize_exp)))
  QC_cv_RSDs[[l]] = mean(rsds)
  
  if(l == 1){
    QC_cv_RSDs[[l]] = paste0(QC_cv_RSDs[[l]], " (raw: ", median(RSD(transform(e, forward = FALSE, lambda = lambda)[[1]][,qc_index])),")")
  }
  
  
  
  
  # # checking batch effect.
  # pca_qc = prcomp(x_left_normalize_exp, scale. = TRUE)
  # plot(pca_qc$x[,1], pca_qc$x[,2])
  # 
  # out = c(order(pca_qc$x[,1], decreasing = TRUE)[1], order(pca_qc$x[,2], decreasing = F)[1:5]) # 136 and 360 are outliers.
  # 
  # 
  # # out_index = unlist(apply(e_qc, 1, function(v){
  # #   out = boxplot.stats(v)$out
  # #   which(v%in%out)
  # # }))
  # 
  # 
  # pca_qc = prcomp(x_left_normalize_exp[-out,], scale. = TRUE) # Batch effect is observed.
  # color = wes_palette("Zissou1", length(pca_qc$x[,1]), type = "continuous")
  # plot(pca_qc$x[,1], pca_qc$x[,2], col = color)
  
  
  # train on validates
  if(with_validates){
    
    x_target_normalize_exps = list()
    
    for(i in 1:length(validates)){
      
      # x_train = x_qc_scale_d
      
      # add noise to input data according to train and target.
      if(layer_units_options$many_training[l]){
        # x_train_input = rbind(t(e_qc), t(e_qc), t(e_qc), t(e_qc), t(e_qc),t(e_qc), t(e_qc), t(e_qc), t(e_qc), t(e_qc))
        # x_train_output = rbind(t(e_qc), t(e_qc), t(e_qc), t(e_qc), t(e_qc),t(e_qc), t(e_qc), t(e_qc), t(e_qc), t(e_qc))
        
        # x_train_output = x_train_input = t(e_qc)[sample(size = many_training_n, 1:nrow(t(e_qc)), replace = TRUE),]
        index = (1:layer_units_options$many_training_n[l])%%nrow(t(e_qc))
        index[index==0] = nrow(t(e_qc))
        x_train_output = x_train_input = t(e_qc)[index,] #sample(size = many_training_n, 1:nrow(t(e_qc)), replace = TRUE)
      }else{
        x_train_input = t(e_qc)
        x_train_output = t(e_qc)
      }
      
      
      
      
      
      # add noise to input data according to train and target.
      x_train_var = apply(x_train_input,2,robust_sd)^2
      target_var = apply(t(e_validates[[i]]),2,robust_sd)^2
      
      for(j in 1:ncol(x_train_input)){
        if(target_var[j] > x_train_var[j]){
          set.seed(j)
          x_train_input[,j] = x_train_input[,j]+rnorm(length(x_train_input[,j]), sd = sqrt(target_var[j] - x_train_var[j])*layer_units_options$noise_weight[l])
          
        }
      }
      
      
      
      
      
      
      
      
      
      
      
      set.seed(42)
      s_index = sample(1:nrow(x_train_input), size = nrow(x_train_input) * 0.8)
      t_index = (1:nrow(x_train_input))[!(1:nrow(x_train_input)) %in% s_index]
      
      x_train_input = t(scale_data(t(x_train_input))[[1]])
      x_train_output = t(scale_data(t(x_train_output))[[1]])
      
      # x_current_test = x_train[t_index,]
      # x_current_train = x_train[s_index,]
      x_current_train_input = x_train_input[s_index,]
      x_current_test_input = x_train_input[t_index,]
      x_current_train_output = x_train_output[s_index,]
      x_current_test_output = x_train_output[t_index,]
      
      
      ae = ae_model(x_current_train_input,x_current_test_input,x_current_train_output,x_current_test_output, layer_units = as.numeric(layer_units_options$x[l]), verbose = 0, patience = layer_units_options$patience[l], 
                    # activations = "selu", 0.1895314
                    # activations = "softmax", 0.3117842
                    activations = as.character(layer_units_options$activation_function[l]), #0.1873987
                    # activations = "softplus", 0.1896158
                    # activations = "softsign",0.2018778
                    # activations = "relu", 0.21986
                    # activations = "tanh", 0.2008602
                    # activations = "sigmoid", 0.2137153
                    # activations = "hard_sigmoid", #0.2183621
                    # activations = "exponential",#error
                    # activations = "linear",0.1832446
                    
                    drop_out_rate = layer_units_options$drop_rates[l], 
                    optimizer = tolower("Adam"),
                    t_index = NULL,
                    s_index = NULL
      )
      
      
      x_target_d = t(e_validates_scale[[i]]$data_scale)
      x_target_sd = e_validates_scale[[i]]$sds
      x_target_mean = e_validates_scale[[i]]$means
      
      
      
      
      # plot_ly(x =1:nrow(x_target_normalize_exp),y = x_target_d[,101],text = 1:nrow(x_target_normalize_exp))
      
      
      
      x_target_predict = predict(ae$final_model, x_target_d)
      
      
      
      
      
      
      
      
      # plot_ly(x = c(1:nrow(x_target_normalize_exp),1:nrow(x_target_normalize_exp)),y = c(x_target_predict[,101],x_target_d[,101]),text = c(1:nrow(x_target_normalize_exp),1:nrow(x_target_normalize_exp)), color = rep(c("pred","true"),each = nrow(x_target_normalize_exp)))
      
      
      
      x_target_normalize = x_target_d
      
      
      # plot(x_train_input[qc_index,2])
      # plot(x = 1:length(x_target_d[,2]), (x_target_d[,2]* x_target_sd[2] + x_target_mean[2]), ylim = c(8,10))
      # points(x = 1:length(x_target_d[,2]),x_target_predict[,2] * min(x_target_sd[2], x_qc_scale_sd[2]) + x_qc_scale_mean[2], col = 'red')
      # points(x = 1:length(x_target_d[,2]),x_target_predict[,2] * x_target_sd[2] + x_target_mean[2], col = 'blue')
      # abline(h = x_target_mean[2], col = 'blue')
      
      
      
      
      # normalize
      for(j in 1:ncol(x_target_normalize)){
        if(layer_units_options$use_qc_sd_when_final_correct[l]){
          x_target_predict[,j] = x_target_predict[,j] * min(x_target_sd[j], x_qc_scale_sd[j]) + x_qc_scale_mean[j]
          # x_target_predict[,j] = x_target_predict[,j] * x_qc_scale_sd[j] + x_qc_scale_mean[j]
        }else{
          x_target_predict[,j] = x_target_predict[,j] * x_target_sd[j] + x_target_mean[j]
        }
        x_target_normalize[,j] = (x_target_normalize[,j]* x_target_sd[j] + x_target_mean[j]) - (x_target_predict[,j] - mean(x_target_predict[,j]))
      }
      
      
      

      
      
      
      # normalize batch effect
      x_target_normalize = t(normalize_batch_effect(t(x_target_normalize), p[p$sampleType %in% validates[i],]$batch))
      
      
      
      # points(x = 1:length(x_target_d[,2]),x_target_normalize[,2], col = 'green')
      
      
      
      e_none = t(transform(e, forward = FALSE, lambda = lambda)[[1]])
      
      # EXP
      # x_target_normalize_exp = exp(x_target_normalize)
      x_target_normalize_exp = t(transform(t(x_target_normalize), forward = FALSE, lambda = lambda)[[1]])
      
      
      
      
      
      # raw_means = apply(exp(e_validates[[i]]),1, mean)
      raw_means = apply(transform(e_validates[[i]], forward = FALSE, lambda = lambda)[[1]],1, mean)
      norm_means = apply(x_target_normalize_exp,2, mean)
      
      for(k in 1:ncol(x_target_normalize_exp)){
        mean_adj = x_target_normalize_exp[,k] - (norm_means[k] - raw_means[k])
        if(any(mean_adj<0) | is.na(any(mean_adj<0))){
          #cat(k,": ",sum(mean_adj<0),"\n")
          if(sum(mean_adj<0)>length(mean_adj)/2 | is.na(sum(mean_adj<0)>length(mean_adj)/2)){
            warning_index[[l]] = c(warning_index[[l]], paste0(validates[i],k))
            # mean_adj = exp(e_validates[[i]])[k,]
            mean_adj = transform(e_validates[[i]], forward = FALSE, lambda = lambda)[[1]][k,]
          }else{
            mean_adj[mean_adj<0] = rnorm(sum(mean_adj<0), mean = min(mean_adj[mean_adj>0], na.rm = TRUE)/2, sd = min(mean_adj[mean_adj>0], na.rm = TRUE)/20)
          }
        }
        x_target_normalize_exp[,k] = mean_adj
      }
      
      # plot(x = 1:length(p$time[validates_indexes[[1]]]),e_none[validates_indexes[[1]],2])
      # points(x_target_normalize_exp[,2], col = 'red')
      
      validates_RSDs[[i]][l] = median(RSD(t(x_target_normalize_exp)))
      if(l == 1){
        validates_RSDs[[i]][l] = paste0(validates_RSDs[[i]][l], "; (raw: ",median(RSD(transform(e[,validates_indexes[[i]]], forward = FALSE, lambda = lambda)[[1]])),")")
      }
      x_target_normalize_exps[[i]] = x_target_normalize_exp
    }
    names(validates_RSDs) = validates
    names(x_target_normalize_exps) = validates
    
  }
  
  
  
  # check batch effect & outilers
  # pca_biorec_before = prcomp(t(exp(e_validates[[1]])), scale. = TRUE)
  # plot_ly(x =pca_biorec_before$x[,1],y = pca_biorec_before$x[,2],text = 1:nrow(pca_biorec_before$x)) %>% layout(title = 'raw data')
  # pca_biorec_after = prcomp(x_target_normalize_exp, scale. = TRUE)
  # plot_ly(x =pca_biorec_after$x[,1],y = pca_biorec_after$x[,2],text = 1:nrow(pca_biorec_after$x)) %>% layout(title = 'normalized data')
  # plot_ly(x =pca_biorec_after$rotation[,1],y = pca_biorec_after$rotation[,2],text = 1:nrow(pca_biorec_after$rotation))
  # plot_ly(x =1:length((e_validates[[1]])[549,]),y = (e_validates[[1]])[549,],text = 1:length(exp(e_validates[[1]])[549,]))
  # plot_ly(x =1:length(x_target_normalize_exp[,549]),y = x_target_normalize_exp[,549],text = 1:length(x_target_normalize_exp[,549]))
  
  
  
  
  
  # before_log = exp(e_validates[[1]])[257,]
  # plot_ly(x =1:length(exp(e_validates[[1]])[257,]),y = before_log,text = 1:length(exp(e_validates[[1]])[257,]))
  # y0 = 0
  # lambda = mean(before_log)^2
  # after_log = log(before_log - y0 + sqrt((before_log-y0)^2 + lambda))
  # plot_ly(x =1:length(exp(e_validates[[1]])[257,]),y = after_log,text = 1:length(exp(e_validates[[1]])[257,]))
  
  
  # try using generalized log transformation.
  # before_log = exp(e_validates[[1]])
  # after_log = before_log
  # y0 = 0
  # for(i in 1:nrow(before_log)){
  # 
  #   lambda = mean(before_log[i,])^2
  #   after_log[i,] = log(before_log[i,] - y0 + sqrt((before_log[i,]-y0)^2 + lambda))
  #   # after_log[i,]  = log(before_log[i,])
  # 
  #   # plot_ly(x = 1:length(before_log[i,]),y = before_log[i,],text = 1:length(before_log[i,]))
  #   # plot_ly(x = 1:length(before_log[i,]),y = after_log[i,],text = 1:length(after_log[i,]))
  # 
  # }
  # pca_biorec_before = prcomp(t(before_log), scale. = TRUE)
  # plot_ly(x =pca_biorec_before$x[,1],y = pca_biorec_before$x[,2],text = 1:nrow(pca_biorec_before$x)) %>% layout(title = 'not transformed')
  # pca_biorec_after = prcomp(t(log(before_log)), scale. = TRUE)
  # plot_ly(x =pca_biorec_after$x[,1],y = pca_biorec_after$x[,2],text = 1:nrow(pca_biorec_after$x)) %>% layout(title = 'log transformed')
  # pca_biorec_after = prcomp(t(after_log), scale. = TRUE)
  # plot_ly(x =pca_biorec_after$x[,1],y = pca_biorec_after$x[,2],text = 1:nrow(pca_biorec_after$x)) %>% layout(title = 'generalized log transformed')
  
  
  
  
  # pca_biorec = prcomp(x_target_normalize_exp, scale. = TRUE)
  # 
  # plot_ly(x =pca_biorec$x[,1],y = pca_biorec$x[,2],text = 1:nrow(pca_biorec$x))
  # 
  # plot_ly(x = pca_biorec$rotation[,1], y = pca_biorec$rotation[,2], text = 1:nrow(pca_biorec$rotation))
  # 
  # plot_ly(x =1:nrow(x_target_normalize_exp),y = x_target_normalize_exp[,101],text = 1:nrow(x_target_normalize_exp))
  # plot_ly(x =1:nrow(x_target_normalize_exp),y = exp(e_validates[[1]][101,]),text = 1:nrow(x_target_normalize_exp))
  
  
  # x_target_normalize_exp[c(105,106),101]
  # exp(e_validates[[1]][101,c(105,106)])
  
  # 
  # out = c(order(pca_biorec$x[,1], decreasing = FALSE)[1:2], order(pca_biorec$x[,2], decreasing = FALSE)[1:3])
  # 
  # 
  # pca_biorec = prcomp(x_target_normalize_exp[-out,], scale. = TRUE) # Batch effect is observed.
  # color = wes_palette("Zissou1", length(pca_biorec$x[,1]), type = "continuous")
  # plot(pca_biorec$x[,1], pca_biorec$x[,2], col = factor(p[p$sampleType=='validate1',]$batch))
  
  
  
  
  # x_train = x_qc_scale_d
  # add noise to input data according to train and target.
  if(layer_units_options$many_training[l]){
    index = (1:layer_units_options$many_training_n[l])%%nrow(t(e_qc))
    index[index==0] = nrow(t(e_qc))
    x_train_output = x_train_input = t(e_qc)[index,] #sample(size = many_training_n, 1:nrow(t(e_qc)), replace = TRUE)
  }else{
    x_train_input = t(e_qc)
    x_train_output = t(e_qc)
  }
  
  
  
  # add noise to input data according to train and target.
  
  
  if(FALSE){
    # j = 1
    j = 1295
    ylim = c(min(c(x_train_input[1:length(qc_index),j],e[j,sample_index])),max(c(x_train_input[1:length(qc_index),j],e[j,sample_index])))
    plot(p_qc$time,x_train_input[1:length(qc_index),j], ylim = ylim)
    points(p$time[sample_index],e[j,sample_index], col = 'red', ylim = ylim)
    
    # Sys.sleep(4)
    
    # 
    # 
    # 
    # cuts = cut(as.numeric(p_qc$time),10)
    # by(x_train_input[1:length(qc_index),j], cuts, sd)
    
    
    
    
    qc_d = x = data.table(x = p$time[qc_index], y = x_train_input[1:length(qc_index),j], y2 = x_train_input[1:length(qc_index),j]^2)
    exp = loess(y~x, data = qc_d, span = 0.1)
    exp_fit = predict(exp, data.frame(x = p$time))
    # exp_fit <- bootstrap::crossval(qc_d$x, qc_d$y, theta.fit = function(x,y){loess(y ~ x)}, theta.predict = function(fit,newdata){predict(fit, newdata = newdata)})$cv.fit
    # exp_fit[1] = exp_fit[2]
    # exp_fit[length(exp_fit)] = exp_fit[length(exp_fit)-1]
    
    exp2 = loess(y2~x, data = qc_d, span = 0.1)
    exp2_fit = predict(exp2, data.frame(x = p$time))
    # exp2_fit <- bootstrap::crossval(qc_d$x, qc_d$y2, theta.fit = function(x,y){loess(y ~ x)}, theta.predict = function(fit,newdata){predict(fit, newdata = newdata)})$cv.fit
    # exp2_fit[1] = exp2_fit[2]
    # exp2_fit[length(exp2_fit)] = exp2_fit[length(exp2_fit)-1]
    var_qc = exp2_fit - exp_fit^2
    
    
    # plot(p_qc$time,x_train_input[1:length(qc_index),j], ylim = ylim)
    # lines(x = p_qc$time, exp_fit)
    # 
    # plot(x_train_input[1:length(qc_index),j] - exp_fit)
    # by(x_train_input[1:length(qc_index),j] - exp_fit, cuts, sd)
    # 
    # 
    # 
    # 
    # plot(p_qc$time,x_train_input[1:length(qc_index),j]^2)
    # lines(x = p_qc$time, exp2_fit)
    # 
    # 
    # plot(p_qc$time, exp2_fit - exp_fit^2)
    # 
    # 
    sample_d = x = data.table(x = p$time[sample_index], y = e[j,sample_index], y2 = e[j,sample_index]^2)
    
    
    exp_sample = loess(y~x, data = sample_d, span = 0.1)
    exp_fit_sample = predict(exp_sample, data.frame(x = p$time))
    # exp_fit_sample <- bootstrap::crossval(sample_d$x, sample_d$y, theta.fit = function(x,y){loess(y ~ x)}, theta.predict = function(fit,newdata){predict(fit, newdata = newdata)})$cv.fit
    # exp_fit_sample[1] = exp_fit_sample[2]
    # exp_fit_sample[length(exp_fit_sample)] = exp_fit_sample[length(exp_fit_sample)-1]
    exp2_sample = loess(y2~x, data = sample_d, span = 0.1)
    exp2_fit_sample = predict(exp2_sample, data.frame(x = p$time))
    # exp2_fit_sample <- bootstrap::crossval(sample_d$x, sample_d$y2, theta.fit = function(x,y){loess(y ~ x)}, theta.predict = function(fit,newdata){predict(fit, newdata = newdata)})$cv.fit
    # exp2_fit_sample[1] = exp2_fit_sample[2]
    # exp2_fit_sample[length(exp2_fit_sample)] = exp2_fit_sample[length(exp2_fit_sample)-1]
    var_sample = exp2_fit_sample - exp_fit_sample^2
    
    
    
    # 
    # 
    
    
    
    plot(p$time, y = sqrt(var_qc))
    points(p$time, sqrt(var_sample), col = 'red')
    
    # sd_var_qc = sqrt(mean((qc_d$y - mean(qc_d$y))^4) - var_qc^2*(length(qc_d$y) - 3)/(length(qc_d$y) * (length(qc_d$y) - 1)))
    
    
    
    
    sd_var_qc = sqrt(2*var_qc^2/(nrow(qc_d)))
    # points(1:length(p$time), sqrt(var_qc + sd_var_qc), type = "l")
    # points(1:length(p$time), sqrt(var_qc - sd_var_qc), type = "l")
    
    sd_var_sample = sqrt(2*var_sample^2/(nrow(qc_d)))
    # points(1:length(p$time), sqrt(var_sample + sd_var_sample), type = "l", col = 'red')
    # points(1:length(p$time), sqrt(var_sample - sd_var_sample), type = "l", col = 'red')
    # 
    # 
    # 
    sample_lower_bound = sqrt(var_sample - sd_var_sample)
    # replace first NAs with first not-NA
    first_not_NA = min(which(!is.na(sample_lower_bound)))
    if(first_not_NA>1){
      sample_lower_bound[1:(first_not_NA-1)] = sample_lower_bound[first_not_NA]
    }
    # replace all other NA's with previous not NA
    while(sum(is.na(sample_lower_bound))>0){
      first_NA = min(which(is.na(sample_lower_bound)))
      sample_lower_bound[first_NA] = sample_lower_bound[first_NA-1]
    }
    
    
    qc_upper_bound = sqrt(var_qc + sd_var_qc)
    
    # replace first NAs with first not-NA
    first_not_NA = min(which(!is.na(qc_upper_bound)))
    if(first_not_NA>1){
      qc_upper_bound[1:(first_not_NA-1)] = qc_upper_bound[first_not_NA]
    }
    # replace all other NA's with previous not NA
    while(sum(is.na(qc_upper_bound))>0){
      first_NA = min(which(is.na(qc_upper_bound)))
      qc_upper_bound[first_NA] = qc_upper_bound[first_NA-1]
    }
    
    greater_or_less = (sample_lower_bound - qc_upper_bound)>0
    which_start_true = which(c(ifelse(greater_or_less[1]>0,1,0),diff(greater_or_less)==1)==1)
    which_start_false = which(c(ifelse(greater_or_less[1]>0,0,1),diff(greater_or_less)==-1)==1)
    
    # abline(v = which_start_true, col = 'red')
    # abline(v = which_start_false, col = 'blue')
    
    greater_intervals = list()
    for(g in 1:length(which_start_true)){
      
      greater_intervals[[g]] = as.numeric(c(which_start_true[g],min(as.numeric(names(which_start_false - which_start_true[g]))[(which_start_false - which_start_true[g])>0])))
      
    }# !!!
    for(g in 1:(length(greater_intervals)-1)){ #1 20; 21 81
      
      if(greater_intervals[[g+1]][1] - greater_intervals[[g]][2] == 1){
        
        greater_intervals[[g]][2] = greater_intervals[[g + 1]][2]
        greater_intervals[[g+1]] = c(0,0)
      }
      
    }
    for(g in length(greater_intervals):1){
      if(identical(greater_intervals[[g]],c(0,0))){
        greater_intervals[[g]] = NULL
      }
    }
    
    # j = j + 1
    
    sample_sds = c()
    qc_sds = c()
    ns = c()
    for(g in 1:length(greater_intervals)){
      
      greater_time = p$time[(p$time < greater_intervals[[g]][2]) & (p$time > greater_intervals[[g]][1])]
      chosen_index_greater = p$time %in% greater_time
      if(g < length(greater_intervals)){
        less_time = p$time[(p$time > greater_intervals[[g]][2]) & (p$time < greater_intervals[[g+1]][1])]
        chosen_index_less = p$time %in% less_time
      }else{
        less_time = p$time[p$time > greater_intervals[[g]][2]]
        chosen_index_less = p$time %in% less_time
      }
      
      
      
      sample_sds[g*2-1] = mean(sqrt(var_sample)[chosen_index_greater], na.rm = TRUE)
      qc_sds[g*2-1] = mean(sqrt(var_qc)[chosen_index_greater], na.rm = TRUE)
      # ns[g*2-1] = sum(chosen_index_greater)
      ns[g*2-1] =  greater_intervals[[g]][2] - greater_intervals[[g]][1]
      
      if(g < length(greater_intervals)){
        sample_sds[g*2] = mean(sqrt(var_sample)[chosen_index_less], na.rm = TRUE)
        qc_sds[g*2] = mean(sqrt(var_qc)[chosen_index_less], na.rm = TRUE)
        # ns[g*2] = sum(chosen_index_less)
        ns[g*2] =  greater_intervals[[g+1]][1] - greater_intervals[[g]][2]
      }
      
    }
    ns[length(ns)] = ns[length(ns)] + 1
    abline(v = sapply(greater_intervals, function(x) p$time[x[1]]), col = 'red')
    abline(v = sapply(greater_intervals, function(x) p$time[x[2]]), col = 'blue')
    
    points(p$time, rep(sample_sds, ns), col = 'green')
  }
 
  
  
  
  
  x_train_var = apply(x_train_input,2,robust_sd)^2
  target_var = apply(t(e[,sample_index]),2,robust_sd)^2
  
  cat("here: ",sum(target_var>x_train_var)/length(target_var))
  
  for(j in 1:ncol(x_train_input)){
    if(target_var[j] > x_train_var[j]){
      set.seed(j)
      x_train_input[,j] = x_train_input[,j]+rnorm(length(x_train_input[,j]), sd = sqrt(target_var[j] - x_train_var[j])*layer_units_options$noise_weight[l])
    }
  }
  
  
  
  set.seed(42)
  s_index = sample(1:nrow(x_train_input), size = nrow(x_train_input) * 0.8)
  t_index = (1:nrow(x_train_input))[!(1:nrow(x_train_input)) %in% s_index]
  
  x_train_input = t(scale_data(t(x_train_input))[[1]])
  x_train_output = t(scale_data(t(x_train_output))[[1]])
  
  # x_current_test = x_train[t_index,]
  # x_current_train = x_train[s_index,]
  x_current_train_input = x_train_input[s_index,]
  x_current_test_input = x_train_input[t_index,]
  x_current_train_output = x_train_output[s_index,]
  x_current_test_output = x_train_output[t_index,]
  
  
  
  
  ae = ae_model(x_current_train_input,x_current_test_input,x_current_train_output,x_current_test_output, layer_units = as.numeric(layer_units_options$x[l]), verbose = 0, patience = layer_units_options$patience[l], 
                # activations = "selu", 0.1895314
                # activations = "softmax", 0.3117842
                activations = as.character(layer_units_options$activation_function[l]), #0.1873987
                # activations = "softplus", 0.1896158
                # activations = "softsign",0.2018778
                # activations = "relu", 0.21986
                # activations = "tanh", 0.2008602
                # activations = "sigmoid", 0.2137153
                # activations = "hard_sigmoid", #0.2183621
                # activations = "exponential",#error
                # activations = "linear",0.1832446
                
                drop_out_rate = layer_units_options$drop_rates[l], 
                optimizer = tolower("Adam"),
                t_index = t_index,
                s_index = s_index
                # ,epochs = 300
  )
  
  
  
  
  
  # x_train_input = x_current_train_input
  # x_test_input = x_current_test_input
  # x_train_output = x_current_train_output
  # x_test_output = x_current_test_output
  
  
  
  # patience = 50; layer_units = c(1400); batch_size = 128; epochs = 2000; verbose = 0;activations = c("elu"); drop_out_rate = 0.05; optimizer = "adam"
  
  
  x_sample_predict = predict(ae$final_model, x_sample_scale_d)
  # 
  # plot(x_sample_scale_d[,1])
  # points(x_sample_predict[,1], col = 'red')
  
  
  
  
  
  x_sample_normalize = x_sample_scale_d
  # normalize
  for(j in 1:ncol(x_sample_normalize)){
    if(layer_units_options$use_qc_sd_when_final_correct[l]){
      x_sample_predict[,j] = x_sample_predict[,j] * min(x_sample_scale_sd[j], x_qc_scale_sd[j]) + x_qc_scale_mean[j]
    }else{
      x_sample_predict[,j] = x_sample_predict[,j] * x_sample_scale_sd[j] + x_sample_scale_mean[j]
    }
  }
  for(j in 1:ncol(x_sample_normalize)){
    x_sample_normalize[,j] = (x_sample_normalize[,j]* x_sample_scale_sd[j] + x_sample_scale_mean[j]) - (x_sample_predict[,j] - mean(x_sample_predict[,j]))
  }
  # normalize batch effect
  x_sample_normalize = t(normalize_batch_effect(t(x_sample_normalize), p$batch[sample_index]))
  
  # EXP
  # x_sample_normalize_exp = exp(x_sample_normalize)
  x_sample_normalize_exp = t(transform(t(x_sample_normalize), forward = FALSE, lambda = lambda)[[1]])
  
  # raw_means = apply(exp(e[,sample_index]),1, mean)
  raw_means = apply(transform(e[,sample_index], forward = FALSE, lambda = lambda)[[1]],1, mean)
  norm_means = apply(x_sample_normalize_exp,2, mean)
  
  transform_temp = transform(e[,sample_index], forward = FALSE, lambda = lambda)[[1]]
  for(k in 1:ncol(x_sample_normalize_exp)){
    mean_adj = x_sample_normalize_exp[,k] - (norm_means[k] - raw_means[k])
    if(any(mean_adj<0) | is.na(any(mean_adj<0))){
      # cat(k,": ",sum(mean_adj<0),"\n")
      if(sum(mean_adj<0)>length(mean_adj)/2 | is.na(sum(mean_adj<0)>length(mean_adj)/2)){
        warning_index[[l]] = c(warning_index[[l]], paste0("sample",k))
        # mean_adj = exp(e[,sample_index])[k,]
        mean_adj = transform_temp[k,]
      }else{
        mean_adj[mean_adj<0] = rnorm(sum(mean_adj<0), mean = min(mean_adj[mean_adj>0], na.rm = TRUE)/2, sd = min(mean_adj[mean_adj>0], na.rm = TRUE)/20)
      }
    }
    x_sample_normalize_exp[,k] = mean_adj
  }
  
  
  
  
  
  # pca_sample = prcomp(x_sample_normalize_exp, scale. = TRUE)
  # plot_ly(x =pca_sample$x[,1],y = pca_sample$x[,2],text = 1:nrow(pca_sample$x))
  # plot_ly(x =pca_sample$rotation[,1],y = pca_sample$rotation[,2],text = 1:nrow(pca_sample$rotation))
  # index = 285
  # 
  # plot_ly(x =1:length(x_sample_normalize_exp[,index]),y = x_sample_normalize_exp[,index],text = 1:length(x_sample_normalize_exp[,index]))
  # plot_ly(x =1:length(e[index,sample_index]),y = e[index,sample_index],text = 1:length(e[index,sample_index]))
  # plot_ly(x =1:length(transform(e, forward = FALSE, lambda = lambda)[[1]][index,sample_index]),y = transform(e, forward = FALSE, lambda = lambda)[[1]][index,sample_index],text = 1:length(transform(e, forward = FALSE, lambda = lambda)[[1]][index,sample_index]))
  # 
  # plot_ly(x =e[index,sample_index],y = x_sample_normalize_exp[,index],text = 1:length(e[index,sample_index]))
  # 
  # # out = c(order(pca_sample$x[,1], decreasing = FALSE)[1:2], order(pca_sample$x[,2], decreasing = FALSE)[1:3])
  # pca_sample = prcomp(x_sample_normalize_exp[-out,], scale. = TRUE) # Batch effect is observed.
  # color = wes_palette("Zissou1", length(pca_sample$x[,1]), type = "continuous")
  # plot(pca_sample$x[,1], pca_sample$x[,2], col = color) # batch effect observed.
  
  if(with_biological_group){
    
    
    
    p_values = apply(x_sample_normalize_exp, 2, function(x){
      oneway.test(x~p$biological_group[sample_index])$p.value
    })
    p_biological_group[l] = sum(p_values<0.05)
    
    
    if(l == 1){
      p_values = apply(t(transform(e, forward = FALSE, lambda = lambda)[[1]]), 2, function(x){
        oneway.test(x~p$biological_group)$p.value
      })
      p_biological_group[l] = paste0(p_biological_group[l]," (raw: ",sum(p_values<0.05),")")
    }
    
    
  }
  
  
  
  
  # sum(apply(transform(e, forward = FALSE, lambda = lambda)[[1]], 1, function(x){
  #   wilcox.test(x~p$biological_group)$p.value
  # }) < 0.05)
  
  
  
  
  
  # aggregate the final normalized dataset.
  # e_norm = exp(e)
  
  
  e_norm = transform(e, forward = FALSE, lambda = lambda)[[1]]
  e_norm[,sample_index] = t(x_sample_normalize_exp)
  if(with_validates){
    for(i in 1:length(validates)){
      e_norm[,p$sampleType %in% validates[i]] = t(x_target_normalize_exps[[validates[i]]])
    }
  }
  
  pred_train = ae$pred_train
  if(layer_units_options$many_training[l]){
    pred_train = pred_train[1:length(qc_index),]
  }
 
  
  
  for(j in 1:ncol(x_qc_scale_d)){
    old = x_qc_scale_d[,j] * x_qc_scale_sd[j] + x_qc_scale_mean[j]
    pred_temp = pred_train[,j] * x_qc_scale_sd[j] + x_qc_scale_mean[j]
    new  = old - (pred_temp - mean(pred_temp))
    # e_norm[,qc_index][j,] = exp(new)
    e_norm[,qc_index][j,] = transform(new, forward = FALSE, lambda = lambda[j])[[1]]
  }
  
  
  
  
  
  # raw_means = apply(exp(e_qc),1, mean)
  raw_means = apply(transform(e_qc, forward = FALSE, lambda = lambda)[[1]],1, mean)
  norm_means = apply(e_norm[,qc_index],1, mean)
  for(k in 1:nrow(e_norm[,qc_index])){
    mean_adj = e_norm[,qc_index][k,] - (norm_means[k] - raw_means[k])
    if(any(mean_adj<0) | is.na(any(mean_adj<0))){
      cat(k,": ",sum(mean_adj<0),"\n")
      if(sum(mean_adj<0)>length(mean_adj)/2 | is.na(sum(mean_adj<0)>length(mean_adj)/2)){
        warning_index[[l]] = c(warning_index[[l]], paste0("aggregating",k))
        # mean_adj = exp(e_qc)[k,]
        mean_adj = transform(e_qc, forward = FALSE, lambda = lambda)[[1]][k,]
      }else{
        mean_adj[mean_adj<0] = rnorm(sum(mean_adj<0), mean = min(mean_adj[mean_adj>0], na.rm = TRUE)/2, sd = min(mean_adj[mean_adj>0], na.rm = TRUE)/20)
      }
      e_norm[,qc_index][k,] = mean_adj
    }
    e_norm[,qc_index][k,] = mean_adj
  }
  
  # e_exp = exp(e)
  e_exp = transform(e, forward = FALSE, lambda = lambda)[[1]]
  sds = apply(e_exp,1,function(x){
    # x = remove_outlier(x)$value
    sd(x)
  })
  sds2 = apply(e_norm,1,function(x){
    # x = remove_outlier(x)$value
    sd(x)
  })
  
  
  
  
  if(with_validates){
    for(i in 1:length(validates)){
      
      validate_index = p$sampleType %in% validates[i]
      
      # plot(x = p$time[validate_index], y = e_none[validate_index,2], ylim = c(0,2500))
      # plot(x = p$time[validate_index], e_norm[2,validate_index], ylim = c(0,2500))
      # 
      # 
      # 
      # points(x = p$time[validate_index], y = ((mean(e_exp[j,validate_index]) - mean(e_exp[j,sample_index]))/sds[j] * sd(e_norm[j,]) + mean(e_norm[j,sample_index]))/mean(e_norm[j,validate_index]) * e_norm[j,validate_index], col = 'red')
      
      
      
      for(j in 1:nrow(e_norm)){
        e_norm[j,validate_index] = ((mean(e_exp[j,validate_index]) - mean(e_exp[j,sample_index]))/sds[j] * sd(e_norm[j,]) + mean(e_norm[j,sample_index]))/mean(e_norm[j,validate_index]) * e_norm[j,validate_index]
      }
      
    }
  }
  
  for(i in 1:nrow(e_norm)){
    e_norm[i,qc_index] = ((mean(e_exp[i,qc_index]) - mean(e_exp[i,sample_index]))/sds[i] * sd(e_norm[i,]) + mean(e_norm[i,sample_index]))/mean(e_norm[i,qc_index]) * e_norm[i,qc_index]
  }
  
  
  
  
  # p$sampleType[real_validate] = "validate1"
  # validates = unique(p$sampleType[grepl("validate", p$sampleType)])
  # validates_indexes = list()
  # for(i in 1:length(validates)){
  #   validates_indexes[[i]] = which(p$sampleType %in% validates[i])
  # }
  # names(validates_indexes) = validates
  
  
  
  
  
  
  
  
  
  png(paste0(dir,"\\",l,"-th PCA.png"))

  sds = apply(e_norm,1,sd)
  if(any(is.na(sds) | any(sds == 0))){

  }
  e_norm_pca_d = e_norm[!is.na(sds) & !sds==0,]
  pca = prcomp(t(e_norm_pca_d), scale. = TRUE)
  plot(pca$x[,1], pca$x[,2], col = factor(p$sampleType, levels = c('sample','qc',validates)))
  dev.off()

  # plot(pca$x[,1], pca$x[,2], col = factor(p$species))
  # 
  # 
  # p$treatment = p$species
  # d <- data.frame(scores1 = pca$x[,1],scores2 = pca$x[,2], label = p$label, index = 1:nrow(p), species = p$species, sex = p$biological_group, treatment = p$treatment, organ = p$organ)
  # 
  # fig <- plot_ly(
  #   d, x = ~scores1, y = ~scores2,
  #   # Hover text:
  #   text = ~paste("sample name: ", label, '$<br>Index:', index, '$<br>Species:', species, '$<br>sex:', sex, '$<br>treatment:', treatment, '$<br>organ:', organ), color = ~treatment
  # )
  # fig
  # 
  # 
  # 
  # d <- data.frame(loading1 = pca$rotation[,1],loading2 = pca$rotation[,2], label = f$label, index = 1:nrow(f))
  # 
  # fig <- plot_ly(
  #   d, x = ~loading1, y = ~loading2,
  #   # Hover text:
  #   text = ~paste("compound name: ", label, '$<br>Index:', index)
  # ) %>%
  #   htmlwidgets::onRender("
  #   function(el) {
  #     el.on('plotly_click', function(d) {
  #       console.log('Click: ', d);
  #     });
  #   }
  # ")
  # 
  # 
  # 
  # 
  # 
  # fig
  # i = 108
  # e_none = transform(e, forward = FALSE, lambda = lambda)[[1]]
  # plot(e_norm[i,]~p$time, col = factor(p$sampleType, levels = c('sample','qc',validates)), main = "after")
  # plot(e_none[i,]~p$time, col = factor(p$sampleType, levels = c('sample','qc',validates)), main = "before")
  
  
  
  
  
  
  
  
  
  
  if(l == 1){
    pca = prcomp(t(transform(e, forward = FALSE, lambda = lambda)[[1]]), scale. = TRUE)
    
    
    # d <- data.frame(scores1 = pca$x[,1],scores2 = pca$x[,2], label = p$label, index = 1:nrow(p), species = p$species, sex = p$biological_group, treatment = p$treatment)
    # 
    # fig <- plot_ly(
    #   d, x = ~scores1, y = ~scores2,
    #   # Hover text:
    #   text = ~paste("sample name: ", label, '$<br>Index:', index, '$<br>Species:', species, '$<br>sex:', sex, '$<br>treatment:', treatment), color = ~treatment
    # )
    # fig
    
    
    plot(pca$x[,1], pca$x[,2], col = factor(p$sampleType, levels = c('sample','qc',validates)))
  }
  e_none =  transform(e, forward = FALSE, lambda = lambda)[[1]]
  for(j in 1:nrow(e_norm)){
    if(!any(is.na(e_norm[j,]))){
      if(any(e_norm[j,]<0)){
        set.seed(j)
        e_norm[j,e_norm[j,]<0] = 1/2 * rnorm(sum(e_norm[j,]<0), mean = min(e_norm[j,e_norm[j,]>0], na.rm = TRUE), sd = min(e_norm[j,e_norm[j,]>0], na.rm = TRUE)*0.1)
      }
    }else{
      e_norm[j,] = e_none[j,]
    }
  }
  
  fwrite(data.table(e_norm),paste0(dir,"\\",l,"-th dataset.csv"))
  
  # sample_index = p$sampleType == 'sample'
  
  if(with_biological_group){
    # Predicting performance.
    # Define training control
    train.control <- trainControl(method = "cv",classProbs = T)
    # Train the model with SERDA
    plda_serda_data = data.frame(y=p$biological_group[sample_index],t(e_norm[,sample_index]))
    model_serda <- train(y ~., data = plda_serda_data, method = "pls",
                         trControl = train.control,preProc = c("center", "scale"))
    
    auc_serda = roc(response = p$biological_group[sample_index], predictor = model_serda$finalModel$fitted.values[,,1][,1])
    auc = auc(auc_serda)
    
    if(l == 1){
      # Train the model with raw data
      plda_raw_data = data.frame(y=p$biological_group[sample_index],t(e_exp[,sample_index]))
      model_raw <- train(y ~., data = plda_raw_data, method = "pls",
                         trControl = train.control,preProc = c("center", "scale"))
      
      auc_raw = roc(response = p$biological_group[sample_index], predictor = model_raw$finalModel$fitted.values[,,1][,1])
      auc = paste0(auc, "; raw(",auc(auc_raw),")")
    }
    
    auc_biological_group[l] = auc
  }
  
  
  
  
  
  calculation_time[l] = Sys.time() - start
  fwrite(data.table(layer_units_options[1:l,],QC_cv_RSDs, do.call("cbind",validates_RSDs),p_biological_group,auc_biological_group,calculation_time),paste0(dir,"\\",'hyperparameter tuning-performance.csv'))
  
  
  if(scatter_plot | l==1){ # time consuming.
    col = factor(p$sampleType, levels = c('sample','qc',validates))
    dots = c(1,16,rep(16, length(validates)))[as.numeric(col)]
    
    cat("Generating scatter plots for each compounds. Please be patient...\n")
    dir.create(paste0(dir,"\\","scatter plots"," - ",l))
    
    normalized = e_norm
    # e_none =  exp(e)
    
    e_none =  transform(e, forward = FALSE, lambda = lambda)[[1]]
    
    for(j in 1:nrow(e_none)){
      png(paste0(dir,"\\","scatter plots - ",l,"\\",j,"th.png"), width = 480*2, height = 480)
      par(mfrow=c(1,2))
      ylim = c(min(e_none[j,]), max(e_none[j,],normalized[j,]))
      if(Inf %in% ylim){
        ylim[2] = max(e_none[j,!is.infinite(e_none[j,])],normalized[j,!is.infinite(normalized[j,])])*1.1
      }
      if(sum(is.na(ylim))<1){
        plot(p$time,e_none[j,], col = factor(p$sampleType, levels = c('sample','qc',validates)), ylim = ylim, main = f$label[j], pch = dots)
        plot(p$time,normalized[j,], col = factor(p$sampleType, levels = c('sample','qc',validates)), ylim = ylim, pch = dots)
      }
      dev.off()
    }
    
  }
}


















comb_p = p



serrf = fread("P20 positive mode - normalization result\\normalized datasets\\normalized by - SERRF.csv")
serrf_data = data.matrix(serrf[,-1])

apply(serrf_data,1,function(x){
  
  t.test(x ~ p$biological_group)$p.value
  
})


serrf_qc_RSD = fread("P20 positive mode - normalization result\\QC - RSD.csv")
serrf_biorec_RSD = fread("P20 positive mode - normalization result\\Validate Samples - Biorec - RSD.csv")


sum(serrf_qc_RSD$SERRF<0.3)/nrow(f)
sum(serrf_biorec_RSD$SERRF<0.3)/nrow(f)






e_exp = exp(e)
breaksList = seq(0.5,1,by=0.01)
selected_index = c(1:length(qc_index))[c(1:length(qc_index)) %% 20]==1
# raw correlation heatmap.
qc_raw = e_exp[,qc_index][,selected_index]
qc_raw_cor = cor(qc_raw, method = 'spearman')
pheatmap(qc_raw_cor, color = colorRampPalette(rev(RColorBrewer::brewer.pal(n = 7, name = "RdYlBu")))(length(breaksList)),breaks = breaksList,show_rownames = F, show_colnames = F)


biorec_raw = e_exp[,biorec_index][,selected_index]
biorec_raw_cor = cor(biorec_raw, method = 'spearman')
pheatmap(biorec_raw_cor, color = colorRampPalette(rev(RColorBrewer::brewer.pal(n = 7, name = "RdYlBu")))(length(breaksList)),breaks = breaksList,show_rownames = F, show_colnames = F)
















# SERRF correlation heatmap.
qc_serrf = data.matrix(fread("SHS4e_GCBinBase_nosum_Zhang_smaller batch&istd_prederrf_20200320 - normalization result\\serrf_cross_validated_qc.csv"))[,selected_index]
qc_serrf_cor = cor(qc_serrf, method = 'spearman')
pheatmap(qc_serrf_cor, color = colorRampPalette(rev(RColorBrewer::brewer.pal(n = 7, name = "RdYlBu")))(length(breaksList)),breaks = breaksList,show_rownames = F, show_colnames = F)


biorec_serrf = serrf_data[,biorec_index][,selected_index]
biorec_serrf_cor = cor(biorec_serrf, method = 'spearman')
pheatmap(biorec_serrf_cor, color = colorRampPalette(rev(RColorBrewer::brewer.pal(n = 7, name = "RdYlBu")))(length(breaksList)),breaks = breaksList,show_rownames = F, show_colnames = F)




# SERDA correlation heatmap.
qc_serda = t(x_left_normalize_exp)[,selected_index]
qc_serda_cor = cor(qc_serda, method = 'spearman')
pheatmap(qc_serda_cor, color = colorRampPalette(rev(RColorBrewer::brewer.pal(n = 7, name = "RdYlBu")))(length(breaksList)),breaks = breaksList,show_rownames = F, show_colnames = F)





biorec_serda = e_norm[,biorec_index][,selected_index]
biorec_serda_cor = cor(biorec_serda, method = 'spearman')
pheatmap(biorec_serda_cor, color = colorRampPalette(rev(RColorBrewer::brewer.pal(n = 7, name = "RdYlBu")))(length(breaksList)),breaks = breaksList,show_rownames = F, show_colnames = F)









# Predicting performance.
# Define training control
train.control <- trainControl(method = "cv",classProbs = T)
# Train the model
plda_serda_data = data.frame(y=p$biological_group[sample_index],t(e_norm[,sample_index]))
model_serda <- train(y ~., data = plda_serda_data, method = "pls",
                     trControl = train.control,preProc = c("center", "scale"))
# Summarize the results
print(model_serda)
# Define training control
# Train the model
plda_serrf_data = data.frame(y=p$biological_group[sample_index],t(serrf_data[,sample_index]))
model_serrf <- train(y ~., data = plda_serrf_data, method = "pls",
                     trControl = train.control,preProc = c("center", "scale"))
# Summarize the results
print(model_serrf)
# Define training control
# Train the model
plda_raw_data = data.frame(y=p$biological_group[sample_index],t(transform(e, forward = FALSE, lambda = lambda)[[1]][,sample_index]))
model_raw <- train(y ~., data = plda_raw_data, method = "pls",
                   trControl = train.control,preProc = c("center", "scale"))
# Summarize the results
print(model_raw)




auc_raw = roc(response = p$biological_group[sample_index], predictor = model_raw$finalModel$fitted.values[,,1][,1])
auc_serrf = roc(response = p$biological_group[sample_index], predictor = model_serrf$finalModel$fitted.values[,,1][,1])
auc_serda = roc(response = p$biological_group[sample_index], predictor = model_serda$finalModel$fitted.values[,,1][,1])


plot(1-auc_raw$specificities,auc_raw$sensitivities, col = "black", main = "predicting performance", xlab = "1 - specificity", ylab = "sensitivity")
points(1-auc_serrf$specificities,auc_serrf$sensitivities, col = "red")
points(1-auc_serda$specificities,auc_serda$sensitivities, col = "gold")
# abline(a = 0, b = 1, ylim = c(0,1))

legend("bottomright", legend = c(paste0("Before Normalization AUC: ", signif(auc(auc_raw),3)), paste("SERRF AUC: ", signif(auc(auc_serrf),3)), paste("SERDA AUC: ", signif(auc(auc_serda),3))), col = c("black","red","gold"), pch = c(16))


auc(auc_serrf)
auc(auc_serda)

# RSD distribution.

p_values = apply(t(serrf_data), 2, function(x){
  t.test(x[sample_index]~p$biological_group[sample_index])$p.value
}) # 288;




raw_QC_RSD = RSD(exp(e_qc))
serda_QC_RSD = RSD(t(x_left_normalize_exp))
serrf_QC_RSD = fread("SHS4e_GCBinBase_nosum_Zhang_smaller batch&istd_prederrf_20200320 - normalization result\\QC - RSD.csv")$SERRF

hist(raw_QC_RSD[raw_QC_RSD>0 & raw_QC_RSD<1],breaks = seq(0,1,by = 0.1))
hist(serrf_QC_RSD[serrf_QC_RSD>0 & serrf_QC_RSD<1],breaks = seq(0,1,by = 0.1))
hist(serda_QC_RSD[serda_QC_RSD>0 & serda_QC_RSD<1],breaks = seq(0,1,by = 0.1))








raw_Biorec_RSD = RSD(exp(e_biorec))
serda_Biorec_RSD = RSD(e_norm[,biorec_index])
serrf_Biorec_RSD = fread("SHS4e_GCBinBase_nosum_Zhang_smaller batch&istd_prederrf_20200320 - normalization result\\Validate Samples - Biorec - RSD.csv")$SERRF


hist(raw_Biorec_RSD[raw_Biorec_RSD>0 & raw_Biorec_RSD<1],breaks = seq(0,1,by = 0.1))
hist(serrf_Biorec_RSD[serrf_Biorec_RSD>0 & serrf_Biorec_RSD<1],breaks = seq(0,1,by = 0.1))
hist(serda_Biorec_RSD[serda_Biorec_RSD>0 & serda_Biorec_RSD<1],breaks = seq(0,1,by = 0.1))

barplot(height = c(median(raw_QC_RSD), median(serrf_QC_RSD), median(serda_QC_RSD)), main = "(cv) QC RSD",ylab = "RSD", names = c("Before Normalization", "SERRF", "SERDA"), ylim = c(0, 0.6), col = c("black","red","gold"))

barplot(height = c(median(raw_Biorec_RSD), median(serrf_Biorec_RSD), median(serda_Biorec_RSD)),ylab = "RSD", main = "Biorec RSD",names = c("Before Normalization", "SERRF", "SERDA"), ylim = c(0, 0.6), col = c("black","red","gold"))







# PCA
col = factor(comb_p$sampleType, levels = c('sample','qc','Biorec'))
dots = c(1,16,rep(16, length(validate_types)))[as.numeric(col)]


pca_raw = prcomp(t(e_exp), scale. = TRUE)
plot(pca_raw$x[,1], pca_raw$x[,2], col = col,pch = dots, main = "Before Normalization", xlab = "PC 1", ylab = "PC 2")
legend("topright", legend = c('sample','qc',"Biorec"), col = c("black","red","green"), pch = c(1,16,16))


pca_serrf = prcomp(t(serrf_data), scale. = TRUE)
plot(pca_serrf$x[,1], pca_serrf$x[,2], col = col,pch = dots, main = "SERRF", xlab = "PC 1", ylab = "PC 2")
legend("topright", legend = c('sample','qc',"Biorec"), col = c("black","red","green"), pch = c(1,16,16))

# e_norm = e_norm_ori
# for(i in 1:nrow(e_norm[,biorec_index])){
#   
#   means = by(e_norm[i,biorec_index],comb_p$`smaller batch`[biorec_index],mean)
#   rep = as.numeric(means)
#   names(rep) = names(means)
#   specific_means = as.numeric(revalue(comb_p$`smaller batch`[biorec_index], rep))
#   
#   e_norm[i,biorec_index] = e_norm[i,biorec_index]/(specific_means/mean(e_norm[i,biorec_index]))
# }
# 
# for(i in 1:nrow(e_norm[,qc_index])){
#   
#   means = by(e_norm[i,qc_index],comb_p$`smaller batch`[qc_index],mean)
#   rep = as.numeric(means)
#   names(rep) = names(means)
#   specific_means = as.numeric(revalue(comb_p$`smaller batch`[qc_index], rep))
#   
#   e_norm[i,qc_index] = e_norm[i,qc_index]/(specific_means/mean(e_norm[i,qc_index]))
# }


pca_serda = prcomp(t(e_norm), scale. = TRUE)
plot(pca_serda$x[,1], pca_serda$x[,2], col = col,pch = dots, main = "SERDA", xlab = "PC 1", ylab = "PC 2")
legend("topright", legend = c('sample','qc',"Biorec"), col = c("black","red","green"), pch = c(1,16,16))



# individual scatter plot
scatter_plot = TRUE
if(scatter_plot){ # time consuming.
  cat("Generating scatter plots for each compounds. Please be patient...\n")
  dir.create(paste0("scatter plots"))
  
  normalized = e_norm
  e_none =  exp(e)
  for(j in 1:nrow(e_none)){
    png(paste0("scatter plots\\",j,"th.png"), width = 480*2, height = 480)
    par(mfrow=c(1,2))
    ylim = c(min(e_none[j,]), max(e_none[j,],normalized[j,]))
    if(Inf %in% ylim){
      ylim[2] = max(e_none[j,!is.infinite(e_none[j,])],normalized[j,!is.infinite(normalized[j,])])*1.1
    }
    if(sum(is.na(ylim))<1){
      plot(comb_p$time,e_none[j,], col = factor(comb_p$sampleType, levels = c('sample','qc',"Biorec")), ylim = ylim, main = f$label[j], pch = dots)
      plot(comb_p$time,normalized[j,], col = factor(comb_p$sampleType, levels = c('sample','qc',"Biorec")), ylim = ylim, pch = dots)
    }
    dev.off()
  }
  
}

j = 27
png(paste0("figure1b.png"), width = 600*2, height = 480)
par(mfrow=c(1,2))

ylim = c(min(e_none[j,]), max(e_none[j,],normalized[j,]))
plot(comb_p$time,e_none[j,], col = factor(comb_p$sampleType, levels = c('sample','qc',"Biorec")), ylim = ylim, main = paste0(f$label[j],"\nNot Normalized"), pch = dots, xlab = "Injection Order", ylab = 'intensity level')
abline(v = which(diff(as.numeric(factor(p$`smaller batch`)))==1), lty=3)
legend("topright", legend = c("batch"), lty = 3)



plot(comb_p$time,normalized[j,], col = factor(comb_p$sampleType, levels = c('sample','qc',"Biorec")), ylim = ylim, pch = dots, main = paste0(f$label[j],"\nNormalized"), xlab = "Injection Order", ylab = 'intensity level')
legend("topright", legend = c('sample','qc',"Biorec"), col = c("black","red","green"), pch = c(1,16,16))
dev.off()

# rationale plot
indexes = c(1,19,23,27,35,38,53,54,65,66,69,77,78,158,163)
for(j in indexes){
  
  png(paste0("rationale_scatter_plot\\",j,".png"), width = 1200*2, height = 320)
  par(mfrow=c(1,2),mar = c(0,0,0,0))
  
  ylim = c(min(e_none[j,]), max(e_none[j,],normalized[j,]))
  plot(comb_p$time,e_none[j,], col = factor(comb_p$sampleType, levels = c('sample','qc',"Biorec")), ylim = ylim, pch = dots, yaxt='n', xaxt = 'n')
  
  
  plot(comb_p$time,normalized[j,], col = factor(comb_p$sampleType, levels = c('sample','qc',"Biorec")), ylim = ylim, pch = dots, yaxt='n', xaxt = 'n')
  legend("topright", legend = c('sample','qc',"Biorec"), col = c("black","red","green"), pch = c(1,16,16))
  dev.off()
  
  
}























